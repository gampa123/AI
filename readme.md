

# ## 100Days of Machine Learning

This project will be updated slowly as required, so stay tuned.

If you have a suggestion, or you want to contribute some code, you are free to make a pull request.

Your contributions will be visible since this project is public.

Find the datasets [here](https://github.com/naveengampala/AI/tree/master/100DayOfMachineLearning/data).

## Articles
- [Introduction to Machine Learning](https://medium.com/@naveengampala/chapter-00-introduction-to-machine-learning-for-beginners-138298507094)

- [Introduction to Linear Regresion](https://medium.com/analytics-vidhya/chapter-01-introduction-to-linear-regression-6285b23c3e66)


## Notebooks
- [Prediction of Quality of Wine -- Support Vector Machine](https://www.kaggle.com/ngmpala/prediction-of-quality-of-wine-svm)

- [Titanic Dataset -- Logistic Regression](https://www.kaggle.com/ngmpala/eda-titanic-dataset)

## Issue with Opening Notebooks
If we are finding issue with open Notebook in github. try to open that notebook that you want using nbviewer online, you don't need to install it,

- open https://nbviewer.jupyter.org/ 
- paste the URL of the Notebook.
## Data Pre-Processing | Day 1
you can check Jupyter Notebook Implementation [here](https://github.com/naveengampala/AI/blob/master/100Days-Of-MachineLearning/Day1/src/Data%20Pre-Processing.ipynb).
- Reading a Dataframe using pandas
- Handles missing values and returns total and percentage 
- First Check the what feature are Categorical Encode using get_dummies pandas function
- Spliting Dataset into Trian and Test using sklearn

## Simple Logistic Regression | Day 2
you can check Jupyter Notebook Implementation [here](https://github.com/naveengampala/AI/blob/master/100Days-Of-MachineLearning/Day2/src/Simple%20Logistic%20Regression.ipynb).

- Introduction to Logistic Regression
- Preprocessing the data essentialy dealing with Missing values
- Logistic regression in scikit-learn

## Simple Linear Regression | Day 3
you can check Jupyter Notebook Implementation [here](https://github.com/naveengampala/AI/blob/master/100Days-Of-MachineLearning/Day3/src/Simple%20Linear%20Regression.ipynb).

- Introduction to Linear Regression
- Preprocessing the data essentialy dealing with Missing values
- Linear regression in scikit-learn
- Root Mean Square Error 

## Exploratory Data Analysis on Prediction of Quality of Wine - Part I | Day 4
you can check Jupyter Notebook Implementation [here](https://github.com/naveengampala/AI/blob/master/100Days-Of-MachineLearning/Day4/src/Exploratory%20Data%20Analysis.ipynb).

- Value of Exploratory Data Analysis
- Distribution of Features
- Visualising Pairwise correlation
- Calculating and plotting heatmap correlation

## Exploratory Data Analysis on Prediction of Quality of Wine - Part II | Day 5


## Support Vector Machine | Day 6


## Implementation of Support Vector Machine | Day 6


## K-Means | Day 7

Let's say we are given Weight and Height of the of the Chimps and Monkey we have to make two dfferent clusters.The Essentail Idea: 

- Distance between the points within a cluster should be Minimized
- Distance of points belonging to different cluster should be maximized

## Implementation of K-Means | Day 8

## Gaussian Discriminant Analysis | Day 9
Last few days we explored a class of Regression models having particularly simple analytical and computational properties 

For Regression problems, The target variable *t* was simply the vector of real numbers, whose values we wish to predict. In the case of Classification. There are various ways of target values to represent class labels.

- For Probabilistic models, the most convenient, In the case of two-class problems is the binary representation in which there is a single target value *t* belongs to *{0,1}* such that *t=1* represents class 1 and *t=0* represents the class 2

- For K>2 classes, It is convenient to use 1-of-K coding scheme in which t is a vector of length K. such that if the class is *C_J* then all elements *t_k* of t are zero except element *t_j* 
 t = {0, 1, 0, 0, 0}^T

Gaussian Discriminant Analysis is a generative approach in which we model class-conditional densities given by 
- P(X | C_k ), together with prior probabilities P (C_k)

## Naive Bayes Algorithm | Day 10

## Implementation of Gaussian Discriminant Analysis | Day 11

## Implementation of Naive Bayes Algorithm | Day 12



## Writing Article on Medium | Day 16 & Day 17
you can check article [here](https://medium.com/@naveengampala/chapter-01-introduction-to-linear-regression-6285b23c3e66)

Linear Regression is a very powerful Machine Learning algorithm that used to calculate a baseline results for predicting future outcomes. <br>

In this article we’ll learn about the following topics:

- Introduction to Linear Regression
- Cost Function
- Applications of Linear Regression
- Implementing Linear Regression with Scikit-Learn
- Pros and Cons
- Summary

## Writing Article on Medium | Day 18 & Day 19
you can check article [here](https://medium.com/analytics-vidhya/chapter-02-introduction-to-logistic-regression-f4750d55ac4a)

In this article we’ll learn about the following topics:

- Introduction to Logistic Regression
- Applications of Logistic Regression
- Mathematics behind Logistic Regression with Scikit-Learn
- Pros and Cons
- Summary

## Writing Article on Medium | Day 20 & Day 21

In this article we’ll learn about the following topics:

- Introduction to Gaussian Discriminant Analysis
- Mathematics behind The Logistic Regression
- Application of Gaussian Discriminant Analysis
- Implementing Gaussian Discriminant Analysis with Scikit-Learn
- Pros and Cons
- Summary

## Writing Article on Medium | Day 22 & Day 23

In this article we’ll learn about the following topics:

- Introduction to Naive Bayes Algorithm
- Mathematics behind The Naive Bayes Algorithm
- Application of Naive Bayes Algorithm
- Implementing Naive Bayes Algorithm with Scikit-Learn
- Pros and Cons
- Summary
## Gram–Schmidt orthogonalization | Day 39 
The Gram-Schmidt iteeration is the basis of one if two principal numerical algorithms for computing QR factorizations. It is a process of traingular orthogonalization making the columns of a matrix orthonormal via a sequence of matrix operated that can be interpreted as Multiplication on the right by UpperTraingular Matrices.

## Feature Selection through Gram–Schmidt orthogonalization | Day 39 
